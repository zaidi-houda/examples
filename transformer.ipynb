{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaidi-houda/examples/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYZ6uijDCGVi",
        "colab_type": "text"
      },
      "source": [
        "### Install the nightly built Tensorflow 2.0 to use LayerNorm layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kElTdoawPxzb",
        "colab_type": "code",
        "outputId": "797f291f-b1d2-4603-f2a2-250259a6c342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "source": [
        "# !pip install tensorflow-gpu==2.0.0-alpha\n",
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/7b/d2dc45e144274961733a18188ea8f67287819c82b02ad0e5d7f0d15cd768/tf_nightly_gpu_2.0_preview-2.0.0.dev20190918-cp36-cp36m-manylinux2010_x86_64.whl (391.5MB)\n",
            "\u001b[K     |████████████████████████████████| 391.5MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/fa/80dd1585bc8b7927229e8465d67010b8e0cc07216922ca74d4b39195527e/tensorflow_estimator_2.0_preview-1.14.0.dev2019091800-py2.py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 31.7MB/s \n",
            "\u001b[?25hCollecting tb-nightly<2.1.0a0,>=2.0.0a0 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/1c/194dd57b4164caf8cf149843c6c240477bd7c8e91c9f7a842ea3f7f114b2/tb_nightly-2.0.0a20190917-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.1.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.1.0a0,>=2.0.0a0->tf-nightly-gpu-2.0-preview) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.1.0a0,>=2.0.0a0->tf-nightly-gpu-2.0-preview) (0.15.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.1.0a0,>=2.0.0a0->tf-nightly-gpu-2.0-preview) (3.1.1)\n",
            "Installing collected packages: tensorflow-estimator-2.0-preview, tb-nightly, tf-nightly-gpu-2.0-preview\n",
            "Successfully installed tb-nightly-2.0.0a20190917 tensorflow-estimator-2.0-preview-1.14.0.dev2019091800 tf-nightly-gpu-2.0-preview-2.0.0.dev20190918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfoKFO4lCOzU",
        "colab_type": "text"
      },
      "source": [
        "### Import and create utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtTr3Pmr9dMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "import time\n",
        "\n",
        "\n",
        "# Mode can be either 'train' or 'infer'\n",
        "# Set to 'infer' will skip the training\n",
        "MODE = 'train'\n",
        "# URL = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "URL = 'http://www.manythings.org/anki/ita-eng.zip'\n",
        "FILENAME = URL[URL.rfind('/'):]\n",
        "\n",
        "\n",
        "def maybe_download_and_read_file(url, filename):\n",
        "    \"\"\" Download and unzip training data\n",
        "    Args:\n",
        "        url: data url\n",
        "        filename: zip filename\n",
        "    \n",
        "    Returns:\n",
        "        Training data: an array containing text lines from the data\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        session = requests.Session()\n",
        "        response = session.get(url, stream=True)\n",
        "\n",
        "        CHUNK_SIZE = 32768\n",
        "        with open(filename, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "    zipf = ZipFile(filename)\n",
        "    filename = zipf.namelist()\n",
        "    with zipf.open(filename[0]) as f:\n",
        "        lines = f.read()\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFCi9xR5CVCZ",
        "colab_type": "text"
      },
      "source": [
        "### Download and process training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYnj3KncAwro",
        "colab_type": "code",
        "outputId": "395cf126-5b82-46a5-c898-14678b955452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "lines = maybe_download_and_read_file(URL, FILENAME)\n",
        "lines = lines.decode('utf-8')\n",
        "\n",
        "raw_data = []\n",
        "for line in lines.split('\\n'):\n",
        "    raw_data.append(line.split('\\t'))\n",
        "\n",
        "print(raw_data[-5:])\n",
        "# The last element is empty, so omit it\n",
        "raw_data = raw_data[:-1]\n",
        "\n",
        "\n",
        "\"\"\"## Preprocessing\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "raw_data_en, raw_data_fr = list(zip(*raw_data))\n",
        "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
        "raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data_fr]\n",
        "raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data_fr]\n",
        "\n",
        "\"\"\"## Tokenization\"\"\"\n",
        "\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
        "                                                        padding='post')\n",
        "\n",
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_in)\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_out)\n",
        "data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,\n",
        "                                                           padding='post')\n",
        "\n",
        "data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,\n",
        "                                                            padding='post')\n",
        "\n",
        "\"\"\"## Create tf.data.Dataset object\"\"\"\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_en, data_fr_in, data_fr_out))\n",
        "dataset = dataset.shuffle(len(data_en)).batch(BATCH_SIZE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ea010d6cb742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download_and_read_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0ca8b6a18a67>\u001b[0m in \u001b[0;36mmaybe_download_and_read_file\u001b[0;34m(url, filename)\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mzipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mzipf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RmU-u7CYwQ",
        "colab_type": "text"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg7_8DsDAZoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Create the Positional Embedding\"\"\"\n",
        "\n",
        "\n",
        "def positional_encoding(pos, model_size):\n",
        "    \"\"\" Compute positional encoding for a particular position\n",
        "    Args:\n",
        "        pos: position of a token in the sequence\n",
        "        model_size: depth size of the model\n",
        "    \n",
        "    Returns:\n",
        "        The positional encoding for the given token\n",
        "    \"\"\"\n",
        "    PE = np.zeros((1, model_size))\n",
        "    for i in range(model_size):\n",
        "        if i % 2 == 0:\n",
        "            PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
        "        else:\n",
        "            PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
        "    return PE\n",
        "\n",
        "max_length = max(len(data_en[0]), len(data_fr_in[0]))\n",
        "MODEL_SIZE = 128\n",
        "\n",
        "pes = []\n",
        "for i in range(max_length):\n",
        "    pes.append(positional_encoding(i, MODEL_SIZE))\n",
        "\n",
        "pes = np.concatenate(pes, axis=0)\n",
        "pes = tf.constant(pes, dtype=tf.float32)\n",
        "\n",
        "\n",
        "print(pes.shape)\n",
        "print(data_en.shape)\n",
        "print(data_fr_in.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJPr_lqxCbvi",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Head Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgug1s0wAeAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Create the Multihead Attention layer\"\"\"\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.Model):\n",
        "    \"\"\" Class for Multi-Head Attention layer\n",
        "    Attributes:\n",
        "        key_size: d_key in the paper\n",
        "        h: number of attention heads\n",
        "        wq: the Linear layer for Q\n",
        "        wk: the Linear layer for K\n",
        "        wv: the Linear layer for V\n",
        "        wo: the Linear layer for the output\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.key_size = model_size // h\n",
        "        self.h = h\n",
        "        self.wq = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
        "        self.wk = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
        "        self.wv = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(value_size) for _ in range(h)]\n",
        "        self.wo = tf.keras.layers.Dense(model_size)\n",
        "\n",
        "    def call(self, query, value, mask=None):\n",
        "        \"\"\" The forward pass for Multi-Head Attention layer\n",
        "        Args:\n",
        "            query: the Q matrix\n",
        "            value: the V matrix, acts as V and K\n",
        "            mask: mask to filter out unwanted tokens\n",
        "                  - zero mask: mask for padded tokens\n",
        "                  - right-side mask: mask to prevent attention towards tokens on the right-hand side\n",
        "        \n",
        "        Returns:\n",
        "            The concatenated context vector\n",
        "            The alignment (attention) vectors of all heads\n",
        "        \"\"\"\n",
        "        # query has shape (batch, query_len, model_size)\n",
        "        # value has shape (batch, value_len, model_size)\n",
        "        query = self.wq(query)\n",
        "        key = self.wk(value)\n",
        "        value = self.wv(value)\n",
        "        \n",
        "        # Split matrices for multi-heads attention\n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        # Originally, query has shape (batch, query_len, model_size)\n",
        "        # We need to reshape to (batch, query_len, h, key_size)\n",
        "        query = tf.reshape(query, [batch_size, -1, self.h, self.key_size])\n",
        "        # In order to compute matmul, the dimensions must be transposed to (batch, h, query_len, key_size)\n",
        "        query = tf.transpose(query, [0, 2, 1, 3])\n",
        "        \n",
        "        # Do the same for key and value\n",
        "        key = tf.reshape(key, [batch_size, -1, self.h, self.key_size])\n",
        "        key = tf.transpose(key, [0, 2, 1, 3])\n",
        "        value = tf.reshape(value, [batch_size, -1, self.h, self.key_size])\n",
        "        value = tf.transpose(value, [0, 2, 1, 3])\n",
        "        \n",
        "        # Compute the dot score\n",
        "        # and divide the score by square root of key_size (as stated in paper)\n",
        "        # (must convert key_size to float32 otherwise an error would occur)\n",
        "        score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
        "        # score will have shape of (batch, h, query_len, value_len)\n",
        "        \n",
        "        # Mask out the score if a mask is provided\n",
        "        # There are two types of mask:\n",
        "        # - Padding mask (batch, 1, 1, value_len): to prevent attention being drawn to padded token (i.e. 0)\n",
        "        # - Look-left mask (batch, 1, query_len, value_len): to prevent decoder to draw attention to tokens to the right\n",
        "        if mask is not None:\n",
        "            score *= mask\n",
        "\n",
        "            # We want the masked out values to be zeros when applying softmax\n",
        "            # One way to accomplish that is assign them to a very large negative value\n",
        "            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
        "        \n",
        "        # Alignment vector: (batch, h, query_len, value_len)\n",
        "        alignment = tf.nn.softmax(score, axis=-1)\n",
        "        \n",
        "        # Context vector: (batch, h, query_len, key_size)\n",
        "        context = tf.matmul(alignment, value)\n",
        "        \n",
        "        # Finally, do the opposite to have a tensor of shape (batch, query_len, model_size)\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])\n",
        "        context = tf.reshape(context, [batch_size, -1, self.key_size * self.h])\n",
        "        \n",
        "        # Apply one last full connected layer (WO)\n",
        "        heads = self.wo(context)\n",
        "        \n",
        "        return heads, alignment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjdTPndICfEu",
        "colab_type": "text"
      },
      "source": [
        "### The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP3f3nloAg5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Create the Encoder\"\"\"\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    \"\"\" Class for the Encoder\n",
        "    Args:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
        "        h: number of attention heads\n",
        "        embedding: Embedding layer\n",
        "        embedding_dropout: Dropout layer for Embedding\n",
        "        attention: array of Multi-Head Attention layers\n",
        "        attention_dropout: array of Dropout layers for Multi-Head Attention\n",
        "        attention_norm: array of LayerNorm layers for Multi-Head Attention\n",
        "        dense_1: array of first Dense layers for FFN\n",
        "        dense_2: array of second Dense layers for FFN\n",
        "        ffn_dropout: array of Dropout layers for FFN\n",
        "        ffn_norm: array of LayerNorm layers for FFN\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "\n",
        "        self.attention_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense_1 = [tf.keras.layers.Dense(\n",
        "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(\n",
        "            model_size) for _ in range(num_layers)]\n",
        "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, sequence, training=True, encoder_mask=None):\n",
        "        \"\"\" Forward pass for the Encoder\n",
        "        Args:\n",
        "            sequence: source input sequences\n",
        "            training: whether training or not (for Dropout)\n",
        "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
        "        \n",
        "        Returns:\n",
        "            The output of the Encoder (batch_size, length, model_size)\n",
        "            The alignment (attention) vectors for all layers\n",
        "        \"\"\"\n",
        "        embed_out = self.embedding(sequence)\n",
        "\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "        embed_out += pes[:sequence.shape[1], :]\n",
        "        embed_out = self.embedding_dropout(embed_out)\n",
        "\n",
        "        sub_in = embed_out\n",
        "        alignments = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            sub_out, alignment = self.attention[i](sub_in, sub_in, encoder_mask)\n",
        "            sub_out = self.attention_dropout[i](sub_out, training=training)\n",
        "            sub_out = sub_in + sub_out\n",
        "            sub_out = self.attention_norm[i](sub_out)\n",
        "            \n",
        "            alignments.append(alignment)\n",
        "            ffn_in = sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
        "            ffn_out = ffn_in + ffn_out\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            sub_in = ffn_out\n",
        "\n",
        "        return ffn_out, alignments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pxeATn-Cxzv",
        "colab_type": "text"
      },
      "source": [
        "### Create an instance of the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-iSrjyYA8ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H = 8\n",
        "NUM_LAYERS = 4\n",
        "vocab_size = len(en_tokenizer.word_index) + 1\n",
        "encoder = Encoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "print(vocab_size)\n",
        "sequence_in = tf.constant([[1, 2, 3, 0, 0]])\n",
        "encoder_output, _ = encoder(sequence_in)\n",
        "encoder_output.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XShJ6Z0mCvyo",
        "colab_type": "text"
      },
      "source": [
        "### The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH6eFjbmBAfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    \"\"\" Class for the Decoder\n",
        "    Args:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
        "        h: number of attention heads\n",
        "        embedding: Embedding layer\n",
        "        embedding_dropout: Dropout layer for Embedding\n",
        "        attention_bot: array of bottom Multi-Head Attention layers (self attention)\n",
        "        attention_bot_dropout: array of Dropout layers for bottom Multi-Head Attention\n",
        "        attention_bot_norm: array of LayerNorm layers for bottom Multi-Head Attention\n",
        "        attention_mid: array of middle Multi-Head Attention layers\n",
        "        attention_mid_dropout: array of Dropout layers for middle Multi-Head Attention\n",
        "        attention_mid_norm: array of LayerNorm layers for middle Multi-Head Attention\n",
        "        dense_1: array of first Dense layers for FFN\n",
        "        dense_2: array of second Dense layers for FFN\n",
        "        ffn_dropout: array of Dropout layers for FFN\n",
        "        ffn_norm: array of LayerNorm layers for FFN\n",
        "        dense: Dense layer to compute final output\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_bot_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.attention_bot_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_mid_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.attention_mid_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense_1 = [tf.keras.layers.Dense(\n",
        "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(\n",
        "            model_size) for _ in range(num_layers)]\n",
        "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, sequence, encoder_output, training=True, encoder_mask=None):\n",
        "        \"\"\" Forward pass for the Decoder\n",
        "        Args:\n",
        "            sequence: source input sequences\n",
        "            encoder_output: output of the Encoder (for computing middle attention)\n",
        "            training: whether training or not (for Dropout)\n",
        "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
        "        \n",
        "        Returns:\n",
        "            The output of the Encoder (batch_size, length, model_size)\n",
        "            The bottom alignment (attention) vectors for all layers\n",
        "            The middle alignment (attention) vectors for all layers\n",
        "        \"\"\"\n",
        "        # EMBEDDING AND POSITIONAL EMBEDDING\n",
        "        embed_out = self.embedding(sequence)\n",
        "\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "        embed_out += pes[:sequence.shape[1], :]\n",
        "        embed_out = self.embedding_dropout(embed_out)\n",
        "\n",
        "        bot_sub_in = embed_out\n",
        "        bot_alignments = []\n",
        "        mid_alignments = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # BOTTOM MULTIHEAD SUB LAYER\n",
        "            seq_len = bot_sub_in.shape[1]\n",
        "\n",
        "            if training:\n",
        "                mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "            else:\n",
        "                mask = None\n",
        "            bot_sub_out, bot_alignment = self.attention_bot[i](bot_sub_in, bot_sub_in, mask)\n",
        "            bot_sub_out = self.attention_bot_dropout[i](bot_sub_out, training=training)\n",
        "            bot_sub_out = bot_sub_in + bot_sub_out\n",
        "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
        "            \n",
        "            bot_alignments.append(bot_alignment)\n",
        "\n",
        "            # MIDDLE MULTIHEAD SUB LAYER\n",
        "            mid_sub_in = bot_sub_out\n",
        "\n",
        "            mid_sub_out, mid_alignment = self.attention_mid[i](\n",
        "                mid_sub_in, encoder_output, encoder_mask)\n",
        "            mid_sub_out = self.attention_mid_dropout[i](mid_sub_out, training=training)\n",
        "            mid_sub_out = mid_sub_out + mid_sub_in\n",
        "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
        "            \n",
        "            mid_alignments.append(mid_alignment)\n",
        "\n",
        "            # FFN\n",
        "            ffn_in = mid_sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
        "            ffn_out = ffn_out + ffn_in\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            bot_sub_in = ffn_out\n",
        "\n",
        "        logits = self.dense(ffn_out)\n",
        "\n",
        "        return logits, bot_alignments, mid_alignments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19DfjeaxCilt",
        "colab_type": "text"
      },
      "source": [
        "### Create an instance of the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDLPEA30BDjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "decoder = Decoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "\n",
        "sequence_in = tf.constant([[14, 24, 36, 0, 0]])\n",
        "decoder_output, _, _ = decoder(sequence_in, encoder_output)\n",
        "decoder_output.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYEswwMC01W",
        "colab_type": "text"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CmFGkJ2BMOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "\n",
        "\n",
        "def loss_func(targets, logits):\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjVxUv2tC3MM",
        "colab_type": "text"
      },
      "source": [
        "### Learning rate scheduling and optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwgwnb-3BNhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\" Learning schedule for training the Transformer\n",
        "    Attributes:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        warmup_steps: number of warmup steps at the beginning\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, warmup_steps=4000):\n",
        "        super(WarmupThenDecaySchedule, self).__init__()\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.model_size = tf.cast(self.model_size, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step_term = tf.math.rsqrt(step)\n",
        "        warmup_term = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)\n",
        "\n",
        "\n",
        "lr = WarmupThenDecaySchedule(MODEL_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(lr,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMvgwyoDC9ks",
        "colab_type": "text"
      },
      "source": [
        "### The predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqSWlTcBPq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_source_text=None):\n",
        "    \"\"\" Predict the output sentence for a given input sentence\n",
        "    Args:\n",
        "        test_source_text: input sentence (raw string)\n",
        "    \n",
        "    Returns:\n",
        "        The encoder's attention vectors\n",
        "        The decoder's bottom attention vectors\n",
        "        The decoder's middle attention vectors\n",
        "        The input string array (input sentence split by ' ')\n",
        "        The output string array\n",
        "    \"\"\"\n",
        "    if test_source_text is None:\n",
        "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
        "    print(test_source_text)\n",
        "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "    print(test_source_seq)\n",
        "\n",
        "    en_output, en_alignments = encoder(tf.constant(test_source_seq), training=False)\n",
        "\n",
        "    de_input = tf.constant(\n",
        "        [[fr_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "    out_words = []\n",
        "\n",
        "    while True:\n",
        "        de_output, de_bot_alignments, de_mid_alignments = decoder(de_input, en_output, training=False)\n",
        "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "        out_words.append(fr_tokenizer.index_word[new_word.numpy()[0][0]])\n",
        "\n",
        "        # Transformer doesn't have sequential mechanism (i.e. states)\n",
        "        # so we have to add the last predicted word to create a new input sequence\n",
        "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
        "\n",
        "        # TODO: get a nicer constraint for the sequence length!\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
        "            break\n",
        "\n",
        "    print(' '.join(out_words))\n",
        "    return en_alignments, de_bot_alignments, de_mid_alignments, test_source_text.split(' '), out_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn4MBXCOC_jB",
        "colab_type": "text"
      },
      "source": [
        "### The train_step function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiUJ7_Y5BSAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(source_seq, target_seq_in, target_seq_out):\n",
        "    \"\"\" Execute one training step (forward pass + backward pass)\n",
        "    Args:\n",
        "        source_seq: source sequences\n",
        "        target_seq_in: input target sequences (<start> + ...)\n",
        "        target_seq_out: output target sequences (... + <end>)\n",
        "    \n",
        "    Returns:\n",
        "        The loss value of the current pass\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_mask = 1 - tf.cast(tf.equal(source_seq, 0), dtype=tf.float32)\n",
        "        # encoder_mask has shape (batch_size, source_len)\n",
        "        # we need to add two more dimensions in between\n",
        "        # to make it broadcastable when computing attention heads\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_output, _ = encoder(source_seq, encoder_mask=encoder_mask)\n",
        "\n",
        "        decoder_output, _, _ = decoder(\n",
        "            target_seq_in, encoder_output, encoder_mask=encoder_mask)\n",
        "\n",
        "        loss = loss_func(target_seq_out, decoder_output)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5H0UbfNDB4S",
        "colab_type": "text"
      },
      "source": [
        "### The training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyXdcoI3BTz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_EPOCHS = 20\n",
        "\n",
        "starttime = time.time()\n",
        "for e in range(NUM_EPOCHS):\n",
        "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "        loss = train_step(source_seq, target_seq_in,\n",
        "                          target_seq_out)\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Elapsed time {:.2f}s'.format(\n",
        "                e + 1, batch, loss.numpy(), time.time() - starttime))\n",
        "            starttime = time.time()\n",
        "\n",
        "    try:\n",
        "        predict()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwddMX_CDEFs",
        "colab_type": "text"
      },
      "source": [
        "### Infer using 20 training examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx-tRr34BY-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sents = (\n",
        "    'What a ridiculous concept!',\n",
        "    'Your idea is not entirely crazy.',\n",
        "    \"A man's worth lies in what he is.\",\n",
        "    'What he did is very wrong.',\n",
        "    \"All three of you need to do that.\",\n",
        "    \"Are you giving me another chance?\",\n",
        "    \"Both Tom and Mary work as models.\",\n",
        "    \"Can I have a few minutes, please?\",\n",
        "    \"Could you close the door, please?\",\n",
        "    \"Did you plant pumpkins this year?\",\n",
        "    \"Do you ever study in the library?\",\n",
        "    \"Don't be deceived by appearances.\",\n",
        "    \"Excuse me. Can you speak English?\",\n",
        "    \"Few people know the true meaning.\",\n",
        "    \"Germany produced many scientists.\",\n",
        "    \"Guess whose birthday it is today.\",\n",
        "    \"He acted like he owned the place.\",\n",
        "    \"Honesty will pay in the long run.\",\n",
        "    \"How do we know this isn't a trap?\",\n",
        "    \"I can't believe you're giving up.\",\n",
        ")\n",
        "\n",
        "for i, test_sent in enumerate(test_sents):\n",
        "    test_sequence = normalize_string(test_sent)\n",
        "    predict(test_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKWm2Vr8Fpa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D_J4P-LDKsx",
        "colab_type": "text"
      },
      "source": [
        "### Infer on a random example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMsRRmu7m2wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_alignments, de_bot_alignments, de_mid_alignments, source, prediction = predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV1_m_YjDPbL",
        "colab_type": "text"
      },
      "source": [
        "### Examine Encoder's attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzfCXDVXm6wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# attention = tf.reduce_mean(de_bot_alignments[0], axis=1).numpy()[0]\n",
        "attention = en_alignments[3][0, 2, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + source, rotation=90)\n",
        "ax.set_yticklabels([''] + source)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia__pcJ4DY-Z",
        "colab_type": "text"
      },
      "source": [
        "### Examine Decoder's lower attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRt7m8PmDdjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention = de_bot_alignments[3][0, 7, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + prediction, rotation=90)\n",
        "ax.set_yticklabels([''] + prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AOVcy73DjWG",
        "colab_type": "text"
      },
      "source": [
        "### Examine Decoder's upper attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbT-_fYsDmLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention = de_mid_alignments[3][0, 2, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + source, rotation=90)\n",
        "ax.set_yticklabels([''] + prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QisUSQUuFz4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}